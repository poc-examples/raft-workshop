{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb19f138-f2fc-4499-bde2-73e47d7e1da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch datasets ipywidgets transformers peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b425ee-1bec-44f1-a507-de69b3fab673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "os.environ[\"PYTORCH_NO_NVML\"] = \"1\"\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c169292-ecf2-45a3-88fd-7dc8d42cb4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = \"surfing\"\n",
    "if ds_name is None:\n",
    "    raise RuntimeError(\"DATASET_NAME not found in .env.state â€“ run 1_gen.ipynb first.\")\n",
    "\n",
    "print(\"Using dataset:\", ds_name)\n",
    "\n",
    "ds_root = Path(\"dataset\") / ds_name\n",
    "files_root = ds_root.parent / f\"{ds_name}-files\"\n",
    "\n",
    "dataset_path_ft_train = files_root / f\"{ds_name}-ft.train.jsonl\"\n",
    "dataset_path_ft_valid = files_root / f\"{ds_name}-ft.valid.jsonl\"\n",
    "\n",
    "print(\"Train path:\", dataset_path_ft_train)\n",
    "print(\"Valid path:\", dataset_path_ft_valid)\n",
    "\n",
    "if not dataset_path_ft_train.is_file():\n",
    "    raise FileNotFoundError(f\"Training file not found: {dataset_path_ft_train}\")\n",
    "if not dataset_path_ft_valid.is_file():\n",
    "    raise FileNotFoundError(f\"Validation file not found: {dataset_path_ft_valid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed00dde-d237-47ab-806c-be8337961c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Training split preview:\")\n",
    "display(pd.read_json(dataset_path_ft_train, lines=True).head(3))\n",
    "\n",
    "print(\"Validation split preview:\")\n",
    "display(pd.read_json(dataset_path_ft_valid, lines=True).head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb650d8-76d0-4e13-8948-17623c6591ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\": str(dataset_path_ft_train),\n",
    "    \"validation\": str(dataset_path_ft_valid),\n",
    "}\n",
    "\n",
    "raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0360a1e-86cc-45b4-a6b3-374706aabcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"ibm-granite/granite-4.0-micro\"\n",
    "MAX_SEQ_LEN = 1024  # you can push higher if your GPU can handle it\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    use_fast=True,\n",
    ")\n",
    "# Granite models use special chat tokens; make sure pad exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ---- Base model load ----\n",
    "# For basic LoRA (no quantization):\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # important for training\n",
    "print(\"Model loaded on:\", model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf2fc9-5763-45b9-ac23-7d8eefa30aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example_to_text(example):\n",
    "    \"\"\"\n",
    "    Convert one RAFT chat example to a single training text string\n",
    "    using Granite's chat template.\n",
    "\n",
    "    example[\"messages\"] is a list of {role, content} dicts:\n",
    "      [{\"role\":\"system\",\"content\":...}, {\"role\":\"user\",...}, {\"role\":\"assistant\",...}, ...]\n",
    "    \"\"\"\n",
    "    messages = example[\"messages\"]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,  # we include assistant answer in the training text\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    text = format_example_to_text(example)\n",
    "    tokenized = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        padding=\"longest\",\n",
    "    )\n",
    "    # Standard causal LM SFT: labels == input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "processed_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    "    desc=\"Tokenizing dataset\",\n",
    ")\n",
    "\n",
    "processed_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eadb92-ce0a-4cb0-a9b1-c7361a939004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3200c3ec-496a-4277-bcb3-b1e6fbcf2bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "OUTPUT_DIR = f\"outputs/granite-4.0-micro-raft-peft-{ds_name}\"\n",
    "\n",
    "train_batch_size = 1   # smallest possible\n",
    "eval_batch_size = 1\n",
    "num_epochs = 1\n",
    "learning_rate = 2e-4\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# rough warmup_steps approximation (if you want to actually use it)\n",
    "num_train_examples = len(processed_datasets[\"train\"])\n",
    "steps_per_epoch = max(num_train_examples // (train_batch_size * 4), 1)  # 4 = grad_accum\n",
    "total_steps = steps_per_epoch * num_epochs\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_steps=warmup_steps,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=torch.cuda.is_available(),\n",
    "    fp16=False,\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_datasets[\"train\"],\n",
    "    eval_dataset=processed_datasets[\"validation\"],  # this is fine; it just won't auto-eval\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be87dd49-34f2-43fc-a1c4-985e27e6ffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c51fab-2ffb-468e-ab31-ac2df61d7914",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39838833-6e3a-4720-b1d1-a680b9d99208",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = f\"outputs/granite-4.0-micro-raft-peft-{ds_name}\"\n",
    "\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(\"Saved adapter + tokenizer to:\", SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626c512-98c4-40ed-9557-55b7d8c987e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
