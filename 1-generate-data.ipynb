{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP Gorilla Raft Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./setup_raft.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install dotenv pandas mdc openai datasets transformers PyPDF2 langchain_experimental langchain_openai azure-identity coloredlogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import ceil\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# LOAD CONFIGURATION\n",
    "# ---------------------------------------------------------------------\n",
    "load_dotenv(\"config.env\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# DATASET CONFIGURATION\n",
    "# ---------------------------------------------------------------------\n",
    "ds_name = os.getenv(\"DATASET_NAME\")\n",
    "ds_file = os.getenv(\"DATASET_FILE\")\n",
    "os.environ[\"DATAFILE_PATH\"] = f\"sample_data/{ds_name}/{ds_file}\"\n",
    "\n",
    "# Define dataset output paths\n",
    "ds_path = f\"dataset/{ds_name}\"\n",
    "os.environ[\"DATASET_OUTPUT_PATH\"] = ds_path\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# TRAINING PARAMETERS\n",
    "# ---------------------------------------------------------------------\n",
    "finetuning_train_split = 0.8\n",
    "finetuning_valid_split = 0.1\n",
    "finetuning_threshold = 65\n",
    "raft_questions = 2\n",
    "qa_threshold = ceil(finetuning_threshold / finetuning_train_split)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# PRINT CONFIGURATION SUMMARY\n",
    "# ---------------------------------------------------------------------\n",
    "print(\n",
    "    f\"\"\"\n",
    "═══════════════════════════════════════════════════════════════════════\n",
    "    RAFT Synthetic Dataset Generation - Configuration Overview\n",
    "═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "MODEL & ROUTER\n",
    "──────────────────────────────────────────────\n",
    " Multi-Model Router URL  : {os.getenv('OPENAI_BASE_URL')}\n",
    " Teacher Model           : {os.getenv('TEACHER_MODEL_ID')}\n",
    " Embedding Model         : {os.getenv('EMBEDDING_MODEL_ID')}\n",
    "\n",
    "DATASET SETUP\n",
    "──────────────────────────────────────────────\n",
    " Dataset Name            : {ds_name}\n",
    " Dataset File            : {ds_file}\n",
    " Training Document Path  : {os.getenv('DATAFILE_PATH')}\n",
    " Output Dataset Path     : {ds_path}\n",
    "\n",
    "TRAINING PARAMETERS\n",
    "──────────────────────────────────────────────\n",
    " Finetuning Train Split  : {finetuning_train_split}\n",
    " Finetuning Valid Split  : {finetuning_valid_split}\n",
    " Finetuning Threshold    : {finetuning_threshold}\n",
    " QA Threshold (derived)  : {qa_threshold}\n",
    " Questions per Chunk     : {raft_questions}\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════════\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 .gorilla/raft/raft.py \\\n",
    "    --datapath $DATAFILE_PATH \\\n",
    "    --output $DATASET_OUTPUT_PATH \\\n",
    "    --doctype pdf \\\n",
    "    --chunk_size 512 \\\n",
    "    --questions 2 \\\n",
    "    --distractors 3 \\\n",
    "    --embedding_model $EMBEDDING_MODEL_ID \\\n",
    "    --completion_model $TEACHER_MODEL_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raft_arrow_file = f\"{ds_path}/data-00000-of-00001.arrow\"\n",
    "os.environ[\"RAFT_ARROW_FILE\"] = raft_arrow_file\n",
    "\n",
    "dataset_path_hf = f\"{ds_path}-files/{ds_name}-hf.full.jsonl\"\n",
    "os.environ[\"DATASET_PATH_HF\"] = dataset_path_hf\n",
    "\n",
    "dataset_path_hf_train = f\"{ds_path}-files/{ds_name}-hf.train.jsonl\"\n",
    "dataset_path_hf_valid = f\"{ds_path}-files/{ds_name}-hf.valid.jsonl\"\n",
    "dataset_path_hf_eval  = f\"{ds_path}-files/{ds_name}-hf.eval.jsonl\"\n",
    "\n",
    "dataset_path_ft_train = f\"{ds_path}-files/{ds_name}-ft.train.jsonl\"\n",
    "dataset_path_ft_valid = f\"{ds_path}-files/{ds_name}-ft.valid.jsonl\"\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "Intermediate Dataset Files\n",
    "--------------------------\n",
    "RAFT arrow file        : {raft_arrow_file}\n",
    "\n",
    "HF JSONL (synthetic data)\n",
    "  Full dataset         : {dataset_path_hf}\n",
    "  Train split          : {dataset_path_hf_train}\n",
    "  Valid split          : {dataset_path_hf_valid}\n",
    "  Eval split           : {dataset_path_hf_eval}\n",
    "\n",
    "Finetuning JSONL (final RAFT-style)\n",
    "  Train split          : {dataset_path_ft_train}\n",
    "  Valid split          : {dataset_path_ft_valid}\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python .gorilla/raft/format.py \\\n",
    "    --input $RAFT_ARROW_FILE \\\n",
    "    --output $DATASET_PATH_HF \\\n",
    "    --output-format hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the synthetic dataset generated in the HF JSONL stage\n",
    "import pandas as pd\n",
    "\n",
    "hf_full_df = pd.read_json(dataset_path_hf, lines=True)\n",
    "hf_full_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display a random sample from the HF dataset to inspect structure and content\n",
    "from IPython.display import display, Markdown\n",
    "from random import randint\n",
    "\n",
    "sample_idx = 2\n",
    "sample = hf_full_df.iloc[sample_idx]\n",
    "\n",
    "instruction_md = sample.instruction.replace(\"<DOCUMENT>\", \"`<DOCUMENT>`\").replace(\"</DOCUMENT>\", \"`</DOCUMENT>`\")\n",
    "oracle_context_md = sample.oracle_context.replace(\"<DOCUMENT>\", \"`<DOCUMENT>`\").replace(\"</DOCUMENT>\", \"`</DOCUMENT>`\")\n",
    "sample_answer_md = sample.cot_answer.replace(\"<ANSWER>\", \"`<ANSWER>`\").replace(\"##begin_quote##\", \"`##begin_quote##`\").replace(\"##end_quote##\", \"`##end_quote##`\")\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "## Oracle Context\n",
    "{oracle_context_md}\n",
    "\n",
    "## Question\n",
    "{sample.question}\n",
    "\n",
    "## CoT Answer\n",
    "{sample_answer_md}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the HF JSONL dataset into train/valid/eval splits and write them to disk\n",
    "import numpy as np\n",
    "\n",
    "samples_count = len(hf_full_df)\n",
    "train_cut = int(finetuning_train_split * samples_count)\n",
    "valid_cut = int((finetuning_train_split + finetuning_valid_split) * samples_count)\n",
    "splits = [train_cut, valid_cut]\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "Splitting HF dataset\n",
    "--------------------\n",
    "Total samples : {samples_count}\n",
    "Train split   : 0 -> {train_cut}        -> {dataset_path_hf_train}\n",
    "Valid split   : {train_cut} -> {valid_cut} -> {dataset_path_hf_valid}\n",
    "Eval split    : {valid_cut} -> {samples_count} -> {dataset_path_hf_eval}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "hf_train_df, hf_valid_df, hf_eval_df = np.split(hf_full_df, splits)\n",
    "\n",
    "hf_train_df.to_json(dataset_path_hf_train, orient=\"records\", lines=True)\n",
    "hf_valid_df.to_json(dataset_path_hf_valid, orient=\"records\", lines=True)\n",
    "hf_eval_df.to_json(dataset_path_hf_eval, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python .gorilla/raft/format.py \\\n",
    "    --input $dataset_path_hf_train \\\n",
    "    --input-type jsonl \\\n",
    "    --output $dataset_path_ft_train \\\n",
    "    --output-format $format \\\n",
    "    --output-completion-prompt-column text\\\n",
    "    --output-completion-completion-column ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python .gorilla/raft/format.py \\\n",
    "    --input $dataset_path_hf_valid \\\n",
    "    --input-type jsonl \\\n",
    "    --output $dataset_path_ft_valid \\\n",
    "    --output-format $format \\\n",
    "    --output-completion-prompt-column text\\\n",
    "    --output-completion-completion-column ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect finetuning and eval splits\n",
    "dataset_path_ft_valid_df = pd.read_json(dataset_path_ft_valid, lines=True)\n",
    "dataset_path_ft_valid_df.head(2)\n",
    "\n",
    "pd.read_json(dataset_path_hf_eval, lines=True).head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
