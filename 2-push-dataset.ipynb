{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21b0ce4-bbff-436f-9dba-0549e27e7e2f",
   "metadata": {},
   "source": [
    "### Install helper libraries for uploads\n",
    "\n",
    "This cell installs a few Python helper tools:\n",
    "\n",
    "- **boto3** – a Python library that lets us talk to our storage system (MinIO/S3) from code.  \n",
    "- **tqdm** – shows a nice progress bar while files are uploading.  \n",
    "- **python-dotenv (`dotenv`)** – reads settings (like bucket names and URLs) from a simple text file called `config.env`.\n",
    "\n",
    "These tools are used in later cells to upload the dataset into the workshop’s storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f781836-458e-4206-8503-e0922aaa426b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3 tqdm dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356b6325-e519-4356-95aa-86d216da0939",
   "metadata": {},
   "source": [
    "### Import Python libraries used for configuration and uploads\n",
    "\n",
    "This cell loads the Python libraries we just installed so we can use them:\n",
    "\n",
    "- **boto3** – to connect to the object storage service (MinIO/S3).  \n",
    "- **os** – to work with files and read environment variables.  \n",
    "- **TransferConfig** – lets us tune how uploads behave (for example, splitting big files into pieces).  \n",
    "- **tqdm** – to show a progress bar while each file uploads.  \n",
    "- **load_dotenv** – to load settings from the `config.env` file.\n",
    "\n",
    "Nothing is uploaded yet; we’re just getting our tools ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6223441-a283-41e3-ba94-02c70650e7d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296feb6a-7a5d-459a-ad96-29990999d0b7",
   "metadata": {},
   "source": [
    "### Load workshop configuration from `config.env`\n",
    "\n",
    "This cell reads settings from the `config.env` file and exposes them to Python as environment variables.\n",
    "\n",
    "- **`config.env`** is a simple text file with lines like `NAME=value`.  \n",
    "- **Environment variables** are just named settings the code can read, such as the project **namespace**.  \n",
    "- The **namespace** is a label that identifies your project in the cluster (for example, `myuser-raft-workshop`). We’ll reuse it to build folder paths in storage.\n",
    "\n",
    "This gives the rest of the notebook the information it needs to know “who you are” and where to store data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ab4b3d-8d30-4e7e-9827-63e41dd15aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# LOAD CONFIGURATION\n",
    "# ---------------------------------------------------------------------\n",
    "load_dotenv(\"config.env\")\n",
    "\n",
    "DATASCIENCE_PROJECT_NAMESPACE = os.getenv('DATASCIENCE_PROJECT_NAMESPACE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a62f6d-b549-4d47-a8a7-69c8d6179fff",
   "metadata": {},
   "source": [
    "### Define where the dataset lives and where it will be uploaded\n",
    "\n",
    "This cell sets:\n",
    "\n",
    "- The **local folder** where your generated dataset is stored (`directory`).  \n",
    "- The **key base** (`key_base`), which is the “folder path” we’ll use inside object storage.  \n",
    "- The **bucket name** (`bucket_name`), read from `AWS_S3_BUCKET` provided by the connection config.\n",
    "\n",
    "> **Bucket? Key?**  \n",
    "> - A **bucket** is like a top-level folder in the storage system.  \n",
    "> - A **key** is the path to a specific file inside that bucket.\n",
    "\n",
    "The cell then prints a short summary showing:\n",
    "\n",
    "- Which local directory will be uploaded.  \n",
    "- Which bucket and path in storage it will go to, in a URL-style form like:  \n",
    "  `s3://<bucket>/<key_base>/`\n",
    "\n",
    "This is your chance to double-check that the upload destination looks right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3571f213-634f-44f3-a781-332f6377f939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "directory = '/opt/app-root/src/raft-workshop/dataset'\n",
    "key_base = f'{DATASCIENCE_PROJECT_NAMESPACE}/dataset'\n",
    "bucket_name = os.getenv(\"AWS_S3_BUCKET\")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "Upload Configuration\n",
    "--------------------\n",
    "Local directory  : {directory}\n",
    "S3 bucket        : {bucket_name}\n",
    "S3 key base      : {key_base}\n",
    "\n",
    "Result:\n",
    "Files from the local dataset directory will be uploaded to:\n",
    "s3://{bucket_name}/{key_base}/\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3a0880-5655-41e3-a2ae-f4e807e98a7f",
   "metadata": {},
   "source": [
    "### Configure efficient uploads for large files\n",
    "\n",
    "This cell tunes how files are uploaded to object storage using `TransferConfig`.\n",
    "\n",
    "Key ideas:\n",
    "\n",
    "- **Multi-part upload** – big files are automatically split into smaller pieces (“chunks”) and uploaded in parts.  \n",
    "  - This makes uploads **faster** and **more reliable**, especially over slower or unstable networks. (like random hotel wifi connections)\n",
    "- **Concurrency / threads** – multiple parts can be uploaded at the same time to speed things up.\n",
    "\n",
    "You don’t need to change these values for the workshop; they’re sensible defaults for uploading dataset files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a5a8ef-c5d9-446d-a023-d504afe2eaa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure S3 transfer settings for efficient multi-part uploads\n",
    "config = TransferConfig(\n",
    "    multipart_threshold=1024 * 25,\n",
    "    max_concurrency=10,\n",
    "    multipart_chunksize=1024 * 25,\n",
    "    use_threads=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee9bc2-9a7d-41d9-9145-c9aacd6105cd",
   "metadata": {},
   "source": [
    "### Upload all dataset files to MinIO with a progress bar\n",
    "\n",
    "This is the main upload step.\n",
    "\n",
    "The cell does the following:\n",
    "\n",
    "1. **Walks through the local dataset folder**  \n",
    "   - Finds every file inside `directory` (including files in subfolders).\n",
    "\n",
    "2. **Builds the storage path (key) for each file**  \n",
    "   - Keeps the same relative folder structure, but under `key_base` in the bucket.\n",
    "\n",
    "3. **Uploads each file to MinIO**  \n",
    "   - Creates a **boto3 client** that connects to the object storage service using the `AWS_S3_ENDPOINT` setting.  \n",
    "   - Uses the connection configuration so large files are uploaded in chunks.  \n",
    "   - Shows a **tqdm progress bar** so you can see how many bytes have been uploaded for each file.\n",
    "\n",
    "4. **Reports success or errors**  \n",
    "   - Prints a message when a file is successfully uploaded.  \n",
    "   - If something goes wrong, it catches the error and prints it instead of crashing the whole notebook.\n",
    "\n",
    "> **MinIO / S3 endpoint**  \n",
    "> - **MinIO** is a storage service used in the workshop that behaves like Amazon’s S3.  \n",
    "> - The **endpoint URL** is simply the web address of that storage service inside the cluster.\n",
    "\n",
    "After this cell finishes, your dataset is safely stored in the workshop’s object storage and ready to be used by other jobs or notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bfb726-2246-49f5-832e-c55bcf918f19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Walk the local dataset directory and upload each file to MinIO with progress bar\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        rel_path = os.path.relpath(file_path, directory)\n",
    "        key_name = os.path.join(key_base, rel_path)\n",
    "\n",
    "        print(key_name)\n",
    "        try:\n",
    "            with tqdm(\n",
    "                total=os.path.getsize(file_path),\n",
    "                unit='B',\n",
    "                unit_scale=True,\n",
    "                desc=file_path\n",
    "            ) as pbar:\n",
    "                s3_client = boto3.client('s3', endpoint_url=os.getenv(\"AWS_S3_ENDPOINT\"))\n",
    "                s3_client.upload_file(\n",
    "                    file_path,\n",
    "                    \"test\",\n",
    "                    key_name,\n",
    "                    Config=config,\n",
    "                    Callback=lambda bytes_transferred: pbar.update(bytes_transferred)\n",
    "                )\n",
    "            print(f'File {file_path} uploaded to {bucket_name}/{key_name}')\n",
    "        except Exception as e:\n",
    "            print(f'Error occurred while uploading {file_path}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae051519-908c-40dc-b4ed-4f5312ee482d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
