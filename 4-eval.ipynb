{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d297e3d7-811d-4eb2-a299-ae94dabaa07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai langchain_openai azure-identity dotenv coloredlogs datasets dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceb81cf-3670-4b0a-8e5e-29982a1f1d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure experiment name, eval file locations, and related env vars\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LOAD CONFIGURATION\n",
    "load_dotenv(\"config.env\")\n",
    "\n",
    "# Basic experiment config\n",
    "experiment_name = \"surfing\"\n",
    "experiment_dir = f\"dataset/{experiment_name}-files\"\n",
    "\n",
    "# Questions generated by 1_gen\n",
    "dataset_path_hf_eval = f\"{experiment_dir}/{experiment_name}-hf.eval.jsonl\"\n",
    "os.environ[\"DATASET_PATH_HF_EVAL\"] = dataset_path_hf_eval\n",
    "\n",
    "# Raw RAFT answer files (model outputs before formatting)\n",
    "dataset_path_hf_eval_answer_baseline = f\"{experiment_dir}/{experiment_name}-hf.eval.answer.baseline.jsonl\"\n",
    "os.environ[\"DATASET_PATH_HF_EVAL_ANSWER_BASELINE\"] = dataset_path_hf_eval_answer_baseline\n",
    "\n",
    "dataset_path_hf_eval_answer_student = f\"{experiment_dir}/{experiment_name}-hf.eval.answer.student.jsonl\"\n",
    "os.environ[\"DATASET_PATH_HF_EVAL_ANSWER_STUDENT\"] = dataset_path_hf_eval_answer_student\n",
    "\n",
    "# Formatted answer files (for scoring)\n",
    "dataset_path_eval_answer_baseline = f\"{experiment_dir}/{experiment_name}-eval.answer.baseline.jsonl\"\n",
    "os.environ[\"DATASET_PATH_EVAL_ANSWER_BASELINE\"] = dataset_path_eval_answer_baseline\n",
    "\n",
    "dataset_path_eval_answer_student = f\"{experiment_dir}/{experiment_name}-eval.answer.student.jsonl\"\n",
    "os.environ[\"DATASET_PATH_EVAL_ANSWER_STUDENT\"] = dataset_path_eval_answer_student\n",
    "\n",
    "# Scored answer files (row-level scores)\n",
    "dataset_path_eval_answer_score_baseline = f\"{experiment_dir}/{experiment_name}-eval.answer.score.baseline.jsonl\"\n",
    "dataset_path_eval_answer_score_student  = f\"{experiment_dir}/{experiment_name}-eval.answer.score.student.jsonl\"\n",
    "\n",
    "# Aggregated metrics files\n",
    "dataset_path_eval_answer_score_metrics_baseline = (\n",
    "    f\"{experiment_dir}/{experiment_name}-eval.answer.score.metrics.baseline.json\"\n",
    ")\n",
    "dataset_path_eval_answer_score_metrics_student = (\n",
    "    f\"{experiment_dir}/{experiment_name}-eval.answer.score.metrics.student.json\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "Evaluation File Configuration\n",
    "-----------------------------\n",
    "Eval questions file                  : {dataset_path_hf_eval}\n",
    "\n",
    "Baseline raw answers (model output)  : {dataset_path_hf_eval_answer_baseline}\n",
    "Student  raw answers (model output)  : {dataset_path_hf_eval_answer_student}\n",
    "\n",
    "Baseline formatted answers (for eval): {dataset_path_eval_answer_baseline}\n",
    "Student  formatted answers (for eval): {dataset_path_eval_answer_student}\n",
    "\n",
    "Baseline row scores JSONL            : {dataset_path_eval_answer_score_baseline}\n",
    "Student  row scores JSONL            : {dataset_path_eval_answer_score_student}\n",
    "\n",
    "Baseline aggregate metrics JSON      : {dataset_path_eval_answer_score_metrics_baseline}\n",
    "Student  aggregate metrics JSON      : {dataset_path_eval_answer_score_metrics_student}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "if not Path(dataset_path_hf_eval).is_file():\n",
    "    raise FileNotFoundError(f\"Eval file not found: {dataset_path_hf_eval}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a48ae9-ef07-4c16-9a70-2567ce159072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure baseline, student, and judge model endpoints (via router)\n",
    "import os\n",
    "\n",
    "OPENAI_BASE_URL   = os.getenv('OPENAI_BASE_URL')\n",
    "OPENAI_API_KEY    = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "BASELINE_MODEL_NAME = \"openai.gpt-oss-120b-1:0\"\n",
    "STUDENT_MODEL_NAME  = \"granite-4.0-micro\"\n",
    "JUDGE_MODEL_NAME    = \"qwen.qwen3-32b-v1:0\"\n",
    "\n",
    "# Baseline model env (used by .gorilla/raft/eval.py with --env-prefix BASELINE)\n",
    "os.environ[\"BASELINE_OPENAI_BASE_URL\"]  = OPENAI_BASE_URL\n",
    "os.environ[\"BASELINE_OPENAI_API_KEY\"]   = OPENAI_API_KEY\n",
    "os.environ[\"BASELINE_OPENAI_DEPLOYMENT\"] = BASELINE_MODEL_NAME\n",
    "os.environ[\"BASELINE_MODEL_API\"]        = \"chat\"\n",
    "\n",
    "# Student model env (used by .gorilla/raft/eval.py with --env-prefix STUDENT)\n",
    "os.environ[\"STUDENT_OPENAI_BASE_URL\"]   = os.getenv('STUDENT_OPENAI_BASE_URL')\n",
    "os.environ[\"STUDENT_OPENAI_API_KEY\"]    = OPENAI_API_KEY\n",
    "os.environ[\"STUDENT_DEPLOYMENT_NAME\"]   = os.getenv('STUDENT_MODEL_ID')\n",
    "os.environ[\"STUDENT_MODEL_API\"]         = \"chat\"\n",
    "\n",
    "# Judge model env (used by judge client via the router)\n",
    "os.environ[\"JUDGE_OPENAI_BASE_URL\"]     = OPENAI_BASE_URL\n",
    "os.environ[\"JUDGE_OPENAI_API_KEY\"]      = OPENAI_API_KEY\n",
    "os.environ[\"JUDGE_OPENAI_DEPLOYMENT\"]   = JUDGE_MODEL_NAME\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "Model Endpoint Configuration\n",
    "----------------------------\n",
    "Baseline endpoint : {os.environ['BASELINE_OPENAI_BASE_URL']}\n",
    "Baseline model    : {BASELINE_MODEL_NAME}\n",
    "\n",
    "Student endpoint  : {os.environ['STUDENT_OPENAI_BASE_URL']}\n",
    "Student model     : {STUDENT_MODEL_NAME}\n",
    "\n",
    "Judge endpoint    : {os.environ['JUDGE_OPENAI_BASE_URL']}\n",
    "Judge model       : {JUDGE_MODEL_NAME}\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57280444-f316-4453-af88-f7cc438dd4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Run baseline model over eval split (if not already done)\n",
    "if [ ! -f \"$DATASET_PATH_HF_EVAL_ANSWER_BASELINE\" ]; then\n",
    "  echo \"Running baseline model over eval split...\"\n",
    "  python .gorilla/raft/eval.py \\\n",
    "    --question-file \"$DATASET_PATH_HF_EVAL\" \\\n",
    "    --answer-file \"$DATASET_PATH_HF_EVAL_ANSWER_BASELINE\" \\\n",
    "    --model \"$BASELINE_OPENAI_DEPLOYMENT\" \\\n",
    "    --env-prefix BASELINE \\\n",
    "    --mode \"$BASELINE_MODEL_API\"\n",
    "else\n",
    "  echo \"Baseline answers file already exists, skipping.\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35216b1c-a0d1-45e7-b878-de605fe1ea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format baseline raw answers into RAFT eval format\n",
    "!python .gorilla/raft/format.py \\\n",
    "    --input \"$DATASET_PATH_HF_EVAL_ANSWER_BASELINE\" \\\n",
    "    --input-type jsonl \\\n",
    "    --output \"$DATASET_PATH_EVAL_ANSWER_BASELINE\" \\\n",
    "    --output-format eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea157f4-fe6f-472d-9e7b-21e8ab001eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities to pretty-print an eval row as Markdown for manual inspection\n",
    "import pandas as pd\n",
    "\n",
    "def row_to_markdown(df, idx):\n",
    "    sample = df.iloc[idx]\n",
    "    md = \"\"\n",
    "    for name in df.columns.values:\n",
    "        value = sample[name]\n",
    "        value = value.replace(\"<DOCUMENT>\", \"`<DOCUMENT>`\").replace(\"</DOCUMENT>\", \"`</DOCUMENT>`\")\n",
    "        value = value.replace(\"<ANSWER>\", \"`<ANSWER>`\").replace(\"...</ANSWER>\", \"`</ANSWER>`\")\n",
    "        value = value.replace(\"##begin_quote##\", \"`##begin_quote##`\").replace(\"##end_quote##\", \"`##end_quote##`\")\n",
    "        md += \"### \" + name + \"\\n\" + value + \"\\n\"\n",
    "    return md\n",
    "\n",
    "def pretty_print_row(df, idx):\n",
    "    from IPython.display import display, Markdown\n",
    "    display(Markdown(row_to_markdown(df, idx)))\n",
    "\n",
    "print(\"Baseline (formatted) answer example:\")\n",
    "pretty_print_row(pd.read_json(dataset_path_eval_answer_baseline, lines=True), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ca4ec6-4b4e-42b0-a772-fe5683edcedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Run student model over eval split (if not already done)\n",
    "if [ ! -f \"$DATASET_PATH_HF_EVAL_ANSWER_STUDENT\" ]; then\n",
    "  echo \"Running student model over eval split...\"\n",
    "  python .gorilla/raft/eval.py \\\n",
    "    --question-file \"$DATASET_PATH_HF_EVAL\" \\\n",
    "    --answer-file \"$DATASET_PATH_HF_EVAL_ANSWER_STUDENT\" \\\n",
    "    --model \"$STUDENT_DEPLOYMENT_NAME\" \\\n",
    "    --env-prefix STUDENT \\\n",
    "    --mode \"$STUDENT_MODEL_API\"\n",
    "else\n",
    "  echo \"Student answers file already exists, skipping.\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c6654-c887-489d-97f1-ef3ce69b2c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few student raw answers\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_json(dataset_path_hf_eval_answer_student, lines=True).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad0fbcd-51ec-423d-a708-b7e8b43cd942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format student raw answers into RAFT eval format\n",
    "! python .gorilla/raft/format.py \\\n",
    "    --input \"$DATASET_PATH_HF_EVAL_ANSWER_STUDENT\" \\\n",
    "    --input-type jsonl \\\n",
    "    --output \"$DATASET_PATH_EVAL_ANSWER_STUDENT\" \\\n",
    "    --output-format eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac58c13-9ea8-4d2d-bed9-2d0f38d3ce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty-print one student formatted answer for inspection\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Student (formatted) answer example:\")\n",
    "pretty_print_row(pd.read_json(dataset_path_eval_answer_student, lines=True), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bb73ee-0ce4-4661-959c-1e2ff9e6595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure judge client (Qwen via router) using OpenAI-compatible API\n",
    "from openai import OpenAI\n",
    "\n",
    "JUDGE_BASE_URL = OPENAI_BASE_URL\n",
    "JUDGE_API_KEY  = OPENAI_API_KEY\n",
    "JUDGE_MODEL    = \"qwen.qwen3-32b-v1:0\"\n",
    "\n",
    "print(\"Judge base URL:\", JUDGE_BASE_URL)\n",
    "print(\"Judge model:   \", JUDGE_MODEL)\n",
    "\n",
    "judge_client = OpenAI(\n",
    "    base_url=JUDGE_BASE_URL,\n",
    "    api_key=JUDGE_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3f40b8-5c77-4c59-9f7e-a16d0554e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: extract the first JSON object from a string (if judge wraps it)\n",
    "def extract_json_block(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Try to pull the first {...} JSON object out of a string.\n",
    "    Useful if the judge wraps JSON in extra text or backticks.\n",
    "    \"\"\"\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    if start == -1 or end == -1 or end <= start:\n",
    "        raise ValueError(f\"Could not find JSON object in: {text!r}\")\n",
    "    return text[start : end + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b99831-82f6-4c70-b131-49867fadc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the judge model once to score a single QA pair\n",
    "def judge_single(question: str, context: str, gold: str, answer: str) -> dict:\n",
    "    \"\"\"\n",
    "    Call the judge model (Qwen via the router) to score one answer.\n",
    "\n",
    "    Returns a dict with keys like:\n",
    "      - \"coherence.gpt_coherence\"\n",
    "      - \"relevance.gpt_relevance\"\n",
    "      - \"groundedness.gpt_groundedness\"\n",
    "      - \"fluency.gpt_fluency\"\n",
    "      - \"similarity.gpt_similarity\"\n",
    "    \"\"\"\n",
    "\n",
    "    system_msg = (\n",
    "        \"You are an impartial evaluation assistant. \"\n",
    "        \"Given a question, context, reference answer, and model answer, \"\n",
    "        \"you will score the model answer on several dimensions from 1 to 5.\\n\\n\"\n",
    "        \"Return ONLY a valid JSON object, no commentary, in this format:\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"coherence\":   <number 1-5>,\\n'\n",
    "        '  \"relevance\":   <number 1-5>,\\n'\n",
    "        '  \"groundedness\":<number 1-5>,\\n'\n",
    "        '  \"fluency\":     <number 1-5>,\\n'\n",
    "        '  \"similarity\":  <number 1-5>\\n'\n",
    "        \"}\\n\"\n",
    "        \"Use whole or decimal numbers (e.g., 3 or 4.5).\"\n",
    "    )\n",
    "\n",
    "    user_msg = f\"\"\"\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context (information to base the answer on):\n",
    "{context}\n",
    "\n",
    "Reference answer (ground truth):\n",
    "{gold}\n",
    "\n",
    "Model answer to evaluate:\n",
    "{answer}\n",
    "\"\"\"\n",
    "\n",
    "    resp = judge_client.chat.completions.create(\n",
    "        model=JUDGE_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=256,\n",
    "    )\n",
    "\n",
    "    text = resp.choices[0].message.content.strip()\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        data = json.loads(extract_json_block(text))\n",
    "\n",
    "    # Map to the same naming style as the original RAFT/Azure notebook\n",
    "    scores = {\n",
    "        \"coherence.gpt_coherence\":       float(data[\"coherence\"]),\n",
    "        \"relevance.gpt_relevance\":       float(data[\"relevance\"]),\n",
    "        \"groundedness.gpt_groundedness\": float(data[\"groundedness\"]),\n",
    "        \"fluency.gpt_fluency\":           float(data[\"fluency\"]),\n",
    "        \"similarity.gpt_similarity\":     float(data[\"similarity\"]),\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62b189d-aca9-44d1-8001-eb6e341244b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score a full formatted dataset file and write row-level + aggregate metrics\n",
    "def score_dataset_file(\n",
    "    formatted_answers_path: str,\n",
    "    row_scores_output_path: str,\n",
    "    metrics_output_path: str,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    formatted_answers_path: JSONL from .gorilla/raft/format.py (eval format)\n",
    "    row_scores_output_path: JSONL where we write per-row inputs & outputs\n",
    "    metrics_output_path:    JSON file with aggregated mean scores\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_json(formatted_answers_path, lines=True)\n",
    "    print(f\"Loaded {len(df)} examples from {formatted_answers_path}\")\n",
    "\n",
    "    rows_out = []\n",
    "    metric_sums = None\n",
    "    n = 0\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        question = row[\"question\"]\n",
    "        context = row.get(\"context\", \"\")\n",
    "        gold    = row[\"gold_final_answer\"]\n",
    "        answer  = row[\"final_answer\"]\n",
    "\n",
    "        scores = judge_single(question, context, gold, answer)\n",
    "\n",
    "        # Initialize metric sums\n",
    "        if metric_sums is None:\n",
    "            metric_sums = {k: 0.0 for k in scores.keys()}\n",
    "\n",
    "        for k, v in scores.items():\n",
    "            metric_sums[k] += v\n",
    "\n",
    "        rows_out.append(\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                    \"question\":          question,\n",
    "                    \"context\":           context,\n",
    "                    \"final_answer\":      answer,\n",
    "                    \"gold_final_answer\": gold,\n",
    "                },\n",
    "                \"outputs\": scores,\n",
    "            }\n",
    "        )\n",
    "        n += 1\n",
    "\n",
    "    # Write row-level scores\n",
    "    with open(row_scores_output_path, \"w\") as f:\n",
    "        for row in rows_out:\n",
    "            f.write(json.dumps(row) + \"\\n\")\n",
    "    print(\"Wrote row scores to:\", row_scores_output_path)\n",
    "\n",
    "    # Compute aggregate metrics\n",
    "    metrics = {k: (v / n if n > 0 else 0.0) for k, v in metric_sums.items()}\n",
    "    with open(metrics_output_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(\"Wrote aggregate metrics to:\", metrics_output_path)\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0804914d-4cc4-4931-9cef-6808ef0608ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run judge over baseline formatted answers and compute metrics\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "baseline_metrics = score_dataset_file(\n",
    "    formatted_answers_path=dataset_path_eval_answer_baseline,\n",
    "    row_scores_output_path=dataset_path_eval_answer_score_baseline,\n",
    "    metrics_output_path=dataset_path_eval_answer_score_metrics_baseline,\n",
    ")\n",
    "\n",
    "print(\"\\nBaseline metrics (judge model):\")\n",
    "for k, v in baseline_metrics.items():\n",
    "    print(f\"{k}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf00e1ea-330a-4cb2-86f0-d81ff29b345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run judge over student formatted answers and compute metrics\n",
    "student_metrics = score_dataset_file(\n",
    "    formatted_answers_path=dataset_path_eval_answer_student,\n",
    "    row_scores_output_path=dataset_path_eval_answer_score_student,\n",
    "    metrics_output_path=dataset_path_eval_answer_score_metrics_student,\n",
    ")\n",
    "\n",
    "print(\"\\nStudent metrics (judge model):\")\n",
    "for k, v in student_metrics.items():\n",
    "    print(f\"{k}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8df9ba-b985-4083-aac2-b29122be0424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs student metrics and compute relative improvement\n",
    "import pandas as pd\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"baseline\": baseline_metrics,\n",
    "        \"student\":  student_metrics,\n",
    "    }\n",
    ")\n",
    "\n",
    "metrics_df[\"improvement\"] = (metrics_df[\"student\"] - metrics_df[\"baseline\"]) / metrics_df[\"baseline\"]\n",
    "\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b7902c-781b-4428-99cd-1a7a39ab2fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
