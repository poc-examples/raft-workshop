{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "231f74f5-381a-4f95-b0ef-3a5b13e27a17",
   "metadata": {},
   "source": [
    "### Install helper libraries for uploads\n",
    "\n",
    "This cell installs a few Python helper tools:\n",
    "\n",
    "- **boto3** – a Python library that lets us talk to our storage system (MinIO) from code.  \n",
    "- **tqdm** – shows a nice progress bar while files are uploading.  \n",
    "- **python-dotenv (`dotenv`)** – reads settings (like bucket names and URLs) from a simple text file called `config.env`.\n",
    "\n",
    "These tools are used in later cells to upload the dataset into the workshop’s storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d297e3d7-811d-4eb2-a299-ae94dabaa07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai langchain_openai azure-identity dotenv coloredlogs datasets dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ed190-a9aa-4749-a593-dc8e1ab0d16b",
   "metadata": {},
   "source": [
    "### Configure evaluation files and paths\n",
    "\n",
    "This cell sets up where all of our evaluation files live and gives them clear names:\n",
    "\n",
    "- **Experiment name**: here it’s `\"surfing\"` – used to build file paths.\n",
    "- **Eval questions file**: the questions generated in notebook `1-generate-data`.\n",
    "- **Raw answers**:\n",
    "  - **Baseline raw answers** – what the original (teacher) model answered.\n",
    "  - **Student raw answers** – what our fine-tuned model answered.\n",
    "- **Formatted answers**:\n",
    "  - Files where answers are put into a standard **RAFT eval format** so they’re easy to score.\n",
    "- **Score files**:\n",
    "  - **Row scores (JSONL)** – one line per question, with its scores.\n",
    "  - **Aggregate metrics (JSON)** – overall averages for each score type.\n",
    "\n",
    "> **JSON vs JSONL**  \n",
    "> - **JSON**: a single structured document (often for summaries or settings).  \n",
    "> - **JSONL** (“JSON Lines”): many small JSON objects, one per line (great for datasets).\n",
    "\n",
    "At the end, it prints a summary of all file paths and checks that the eval questions file actually exists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceb81cf-3670-4b0a-8e5e-29982a1f1d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure experiment name, eval file locations, and related env vars\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LOAD CONFIGURATION\n",
    "load_dotenv(\"config.env\")\n",
    "\n",
    "# Basic experiment config\n",
    "experiment_name = \"surfing\"\n",
    "experiment_dir = f\"dataset/{experiment_name}-files\"\n",
    "\n",
    "# Questions generated by 1_gen\n",
    "dataset_path_hf_eval = f\"{experiment_dir}/{experiment_name}-hf.eval.jsonl\"\n",
    "os.environ[\"DATASET_PATH_HF_EVAL\"] = dataset_path_hf_eval\n",
    "\n",
    "# Raw RAFT answer files (model outputs before formatting)\n",
    "dataset_path_hf_eval_answer_baseline = f\"{experiment_dir}/{experiment_name}-hf.eval.answer.baseline.jsonl\"\n",
    "os.environ[\"DATASET_PATH_HF_EVAL_ANSWER_BASELINE\"] = dataset_path_hf_eval_answer_baseline\n",
    "\n",
    "dataset_path_hf_eval_answer_student = f\"{experiment_dir}/{experiment_name}-hf.eval.answer.student.jsonl\"\n",
    "os.environ[\"DATASET_PATH_HF_EVAL_ANSWER_STUDENT\"] = dataset_path_hf_eval_answer_student\n",
    "\n",
    "# Formatted answer files (for scoring)\n",
    "dataset_path_eval_answer_baseline = f\"{experiment_dir}/{experiment_name}-eval.answer.baseline.jsonl\"\n",
    "os.environ[\"DATASET_PATH_EVAL_ANSWER_BASELINE\"] = dataset_path_eval_answer_baseline\n",
    "\n",
    "dataset_path_eval_answer_student = f\"{experiment_dir}/{experiment_name}-eval.answer.student.jsonl\"\n",
    "os.environ[\"DATASET_PATH_EVAL_ANSWER_STUDENT\"] = dataset_path_eval_answer_student\n",
    "\n",
    "# Scored answer files (row-level scores)\n",
    "dataset_path_eval_answer_score_baseline = f\"{experiment_dir}/{experiment_name}-eval.answer.score.baseline.jsonl\"\n",
    "dataset_path_eval_answer_score_student  = f\"{experiment_dir}/{experiment_name}-eval.answer.score.student.jsonl\"\n",
    "\n",
    "# Aggregated metrics files\n",
    "dataset_path_eval_answer_score_metrics_baseline = (\n",
    "    f\"{experiment_dir}/{experiment_name}-eval.answer.score.metrics.baseline.json\"\n",
    ")\n",
    "dataset_path_eval_answer_score_metrics_student = (\n",
    "    f\"{experiment_dir}/{experiment_name}-eval.answer.score.metrics.student.json\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "Evaluation File Configuration\n",
    "-----------------------------\n",
    "Eval questions file                  : {dataset_path_hf_eval}\n",
    "\n",
    "Baseline raw answers (model output)  : {dataset_path_hf_eval_answer_baseline}\n",
    "Student  raw answers (model output)  : {dataset_path_hf_eval_answer_student}\n",
    "\n",
    "Baseline formatted answers (for eval): {dataset_path_eval_answer_baseline}\n",
    "Student  formatted answers (for eval): {dataset_path_eval_answer_student}\n",
    "\n",
    "Baseline row scores JSONL            : {dataset_path_eval_answer_score_baseline}\n",
    "Student  row scores JSONL            : {dataset_path_eval_answer_score_student}\n",
    "\n",
    "Baseline aggregate metrics JSON      : {dataset_path_eval_answer_score_metrics_baseline}\n",
    "Student  aggregate metrics JSON      : {dataset_path_eval_answer_score_metrics_student}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "if not Path(dataset_path_hf_eval).is_file():\n",
    "    raise FileNotFoundError(f\"Eval file not found: {dataset_path_hf_eval}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dd56f6-ed8d-4ea3-8b90-2d717e2c1318",
   "metadata": {},
   "source": [
    "### Configure baseline, student, and judge model endpoints\n",
    "\n",
    "This cell tells the notebook **which models to talk to** and **where they live**:\n",
    "\n",
    "- Reads a shared **base URL** and **API key** from the environment.\n",
    "- Defines three logical roles:\n",
    "  - **Baseline model** – the original “teacher” model we compare against.\n",
    "  - **Student model** – the fine-tuned Granite model we trained.\n",
    "  - **Judge model** – a strong model (Qwen) used **only to score answers**.\n",
    "\n",
    "It then sets environment variables like:\n",
    "\n",
    "- `BASELINE_OPENAI_BASE_URL`, `BASELINE_OPENAI_API_KEY`, etc.  \n",
    "- `STUDENT_OPENAI_BASE_URL`, `STUDENT_DEPLOYMENT_NAME`, etc.  \n",
    "- `JUDGE_OPENAI_BASE_URL`, `JUDGE_OPENAI_DEPLOYMENT`, etc.\n",
    "\n",
    "> **Endpoint / router**  \n",
    "> - An **endpoint** is the URL where we send our requests.  \n",
    "> - A **router** is a service that receives those requests and forwards them to the right model.\n",
    "\n",
    "Finally, it prints a short summary so you can see which endpoint and model name each role is using.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a48ae9-ef07-4c16-9a70-2567ce159072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure baseline, student, and judge model endpoints (via router)\n",
    "import os\n",
    "\n",
    "OPENAI_BASE_URL   = os.getenv('OPENAI_BASE_URL')\n",
    "OPENAI_API_KEY    = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "BASELINE_MODEL_NAME = \"openai.gpt-oss-120b-1:0\"\n",
    "STUDENT_MODEL_NAME  = \"granite-4.0-micro\"\n",
    "JUDGE_MODEL_NAME    = \"qwen.qwen3-32b-v1:0\"\n",
    "\n",
    "# Baseline model env (used by .gorilla/raft/eval.py with --env-prefix BASELINE)\n",
    "os.environ[\"BASELINE_OPENAI_BASE_URL\"]  = OPENAI_BASE_URL\n",
    "os.environ[\"BASELINE_OPENAI_API_KEY\"]   = OPENAI_API_KEY\n",
    "os.environ[\"BASELINE_OPENAI_DEPLOYMENT\"] = BASELINE_MODEL_NAME\n",
    "os.environ[\"BASELINE_MODEL_API\"]        = \"chat\"\n",
    "\n",
    "# Student model env (used by .gorilla/raft/eval.py with --env-prefix STUDENT)\n",
    "os.environ[\"STUDENT_OPENAI_BASE_URL\"]   = os.getenv('STUDENT_OPENAI_BASE_URL')\n",
    "os.environ[\"STUDENT_OPENAI_API_KEY\"]    = OPENAI_API_KEY\n",
    "os.environ[\"STUDENT_DEPLOYMENT_NAME\"]   = \"granite-student\"\n",
    "os.environ[\"STUDENT_MODEL_API\"]         = \"chat\"\n",
    "\n",
    "# Judge model env (used by judge client via the router)\n",
    "os.environ[\"JUDGE_OPENAI_BASE_URL\"]     = OPENAI_BASE_URL\n",
    "os.environ[\"JUDGE_OPENAI_API_KEY\"]      = OPENAI_API_KEY\n",
    "os.environ[\"JUDGE_OPENAI_DEPLOYMENT\"]   = JUDGE_MODEL_NAME\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "Model Endpoint Configuration\n",
    "----------------------------\n",
    "Baseline endpoint : {os.environ['BASELINE_OPENAI_BASE_URL']}\n",
    "Baseline model    : {BASELINE_MODEL_NAME}\n",
    "\n",
    "Student endpoint  : {os.environ['STUDENT_OPENAI_BASE_URL']}\n",
    "Student model     : {STUDENT_MODEL_NAME}\n",
    "\n",
    "Judge endpoint    : {os.environ['JUDGE_OPENAI_BASE_URL']}\n",
    "Judge model       : {JUDGE_MODEL_NAME}\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9ff59b-6ddd-4487-aa79-605af4cf1592",
   "metadata": {},
   "source": [
    "### Run the baseline model over the evaluation questions\n",
    "\n",
    "This cell calls the **baseline (teacher) model** to answer every evaluation question.\n",
    "\n",
    "- Uses the RAFT `eval.py` script to:\n",
    "  - Read the eval questions file.\n",
    "  - Send each question to the **baseline model** via the router.\n",
    "  - Save the answers as a **raw JSONL file**.\n",
    "- It only runs if the baseline answers file does **not** already exist, so you don’t waste time re-running the same evaluation.\n",
    "\n",
    "This gives us the baseline’s raw answers, which we’ll later format and score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57280444-f316-4453-af88-f7cc438dd4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Run baseline model over eval split (if not already done)\n",
    "if [ ! -f \"$DATASET_PATH_HF_EVAL_ANSWER_BASELINE\" ]; then\n",
    "  echo \"Running baseline model over eval split...\"\n",
    "  python .gorilla/raft/eval.py \\\n",
    "    --question-file \"$DATASET_PATH_HF_EVAL\" \\\n",
    "    --answer-file \"$DATASET_PATH_HF_EVAL_ANSWER_BASELINE\" \\\n",
    "    --model \"$BASELINE_OPENAI_DEPLOYMENT\" \\\n",
    "    --env-prefix BASELINE \\\n",
    "    --mode \"$BASELINE_MODEL_API\"\n",
    "else\n",
    "  echo \"Baseline answers file already exists, skipping.\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb72ba-d1f5-4ff9-af7a-5d771e3be218",
   "metadata": {},
   "source": [
    "### Convert baseline raw answers into RAFT eval format\n",
    "\n",
    "This cell uses the RAFT `format.py` helper to:\n",
    "\n",
    "- Take the **baseline raw answers JSONL** as input.\n",
    "- Produce a **formatted eval JSONL** file.\n",
    "\n",
    "The **RAFT eval format** organizes each example into a common structure (question, context, gold/reference answer, model’s final answer), which makes it easy for the judge model and scoring code to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35216b1c-a0d1-45e7-b878-de605fe1ea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format baseline raw answers into RAFT eval format\n",
    "!python .gorilla/raft/format.py \\\n",
    "    --input \"$DATASET_PATH_HF_EVAL_ANSWER_BASELINE\" \\\n",
    "    --input-type jsonl \\\n",
    "    --output \"$DATASET_PATH_EVAL_ANSWER_BASELINE\" \\\n",
    "    --output-format eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553b661-23a0-4b91-a1d7-a791b8d0980a",
   "metadata": {},
   "source": [
    "### Helper: nicely display one formatted answer\n",
    "\n",
    "This cell defines two helper functions:\n",
    "\n",
    "- `row_to_markdown(df, idx)` – turns one row from a DataFrame into a readable Markdown block.\n",
    "- `pretty_print_row(df, idx)` – actually displays that Markdown in the notebook.\n",
    "\n",
    "It also:\n",
    "- Cleans up special tags like `<DOCUMENT>`, `<ANSWER>`, and custom quote markers so they display nicely.\n",
    "- Loads the **baseline formatted answers** and prints one example.\n",
    "\n",
    "This lets you eyeball a single example to check that the question, context, and answers all look reasonable before you run large-scale scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea157f4-fe6f-472d-9e7b-21e8ab001eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities to pretty-print an eval row as Markdown for manual inspection\n",
    "import pandas as pd\n",
    "\n",
    "def row_to_markdown(df, idx):\n",
    "    sample = df.iloc[idx]\n",
    "    md = \"\"\n",
    "    for name in df.columns.values:\n",
    "        value = sample[name]\n",
    "        value = value.replace(\"<DOCUMENT>\", \"`<DOCUMENT>`\").replace(\"</DOCUMENT>\", \"`</DOCUMENT>`\")\n",
    "        value = value.replace(\"<ANSWER>\", \"`<ANSWER>`\").replace(\"...</ANSWER>\", \"`</ANSWER>`\")\n",
    "        value = value.replace(\"##begin_quote##\", \"`##begin_quote##`\").replace(\"##end_quote##\", \"`##end_quote##`\")\n",
    "        md += \"### \" + name + \"\\n\" + value + \"\\n\"\n",
    "    return md\n",
    "\n",
    "def pretty_print_row(df, idx):\n",
    "    from IPython.display import display, Markdown\n",
    "    display(Markdown(row_to_markdown(df, idx)))\n",
    "\n",
    "print(\"Baseline (formatted) answer example:\")\n",
    "pretty_print_row(pd.read_json(dataset_path_eval_answer_baseline, lines=True), 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988a7593-833a-48c9-b335-7f184dcc28d8",
   "metadata": {},
   "source": [
    "### Run the student (fine-tuned) model over the evaluation questions\n",
    "\n",
    "This cell does the **same type of evaluation as the baseline**, but for the **student model**:\n",
    "\n",
    "- Calls the RAFT `eval.py` script again.\n",
    "- Sends each eval question to the **student deployment**.\n",
    "- Writes out a **student raw answers JSONL** file.\n",
    "\n",
    "It also skips running if the output file already exists.  \n",
    "Now we have raw answers for both baseline and student on exactly the same questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ca4ec6-4b4e-42b0-a772-fe5683edcedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Run student model over eval split (if not already done)\n",
    "if [ ! -f \"$DATASET_PATH_HF_EVAL_ANSWER_STUDENT\" ]; then\n",
    "  echo \"Running student model over eval split...\"\n",
    "  python .gorilla/raft/eval.py \\\n",
    "    --question-file \"$DATASET_PATH_HF_EVAL\" \\\n",
    "    --answer-file \"$DATASET_PATH_HF_EVAL_ANSWER_STUDENT\" \\\n",
    "    --model \"$STUDENT_DEPLOYMENT_NAME\" \\\n",
    "    --env-prefix STUDENT \\\n",
    "    --mode \"$STUDENT_MODEL_API\"\n",
    "else\n",
    "  echo \"Student answers file already exists, skipping.\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493a2828-3615-436f-96c6-56aeb7d17f3b",
   "metadata": {},
   "source": [
    "### Preview the student model’s raw answers\n",
    "\n",
    "This cell:\n",
    "\n",
    "- Loads the **student raw answers JSONL** into a pandas table.\n",
    "- Shows the first two rows.\n",
    "\n",
    "It’s just a quick spot-check to confirm that the student model produced answers in the expected structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c6654-c887-489d-97f1-ef3ce69b2c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few student raw answers\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_json(dataset_path_hf_eval_answer_student, lines=True).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbfc9d2-db15-4fc0-9458-67d95924459b",
   "metadata": {},
   "source": [
    "### Convert student raw answers into RAFT eval format\n",
    "\n",
    "This cell mirrors what we did for the baseline:\n",
    "\n",
    "- Takes the **student raw answers JSONL** as input.\n",
    "- Uses RAFT `format.py` to create a **student eval JSONL** file in the standard eval format.\n",
    "\n",
    "After this, both baseline and student answers are ready to be scored in exactly the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad0fbcd-51ec-423d-a708-b7e8b43cd942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format student raw answers into RAFT eval format\n",
    "! python .gorilla/raft/format.py \\\n",
    "    --input \"$DATASET_PATH_HF_EVAL_ANSWER_STUDENT\" \\\n",
    "    --input-type jsonl \\\n",
    "    --output \"$DATASET_PATH_EVAL_ANSWER_STUDENT\" \\\n",
    "    --output-format eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2912959-efa6-4345-a11a-c387fa32d220",
   "metadata": {},
   "source": [
    "### Display one student formatted answer for comparison\n",
    "\n",
    "This cell:\n",
    "\n",
    "- Loads the **student formatted answers**.\n",
    "- Uses `pretty_print_row` to render one of them as Markdown.\n",
    "\n",
    "You can compare this visually with the baseline example to see how the student’s reasoning and style differ from the teacher’s on the same question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac58c13-9ea8-4d2d-bed9-2d0f38d3ce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty-print one student formatted answer for inspection\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Student (formatted) answer example:\")\n",
    "pretty_print_row(pd.read_json(dataset_path_eval_answer_student, lines=True), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f0c944-db67-4fb7-a250-f90288331b70",
   "metadata": {},
   "source": [
    "### Set up the judge model client\n",
    "\n",
    "This cell configures the **judge model**, which scores how good each answer is.\n",
    "\n",
    "- Uses the `OpenAI` Python client, pointing it at our internal **router** (`base_url`) with an **API key**.\n",
    "- Chooses **Qwen 32B** as the judge model:\n",
    "  - It doesn’t answer user questions for the app itself.\n",
    "  - Instead, it reads the question, context, gold answer, and model answer, and returns quality scores.\n",
    "\n",
    "It prints the judge URL and model name, then builds a `judge_client` we’ll reuse when scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bb73ee-0ce4-4661-959c-1e2ff9e6595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure judge client (Qwen via router) using OpenAI-compatible API\n",
    "from openai import OpenAI\n",
    "\n",
    "JUDGE_BASE_URL = OPENAI_BASE_URL\n",
    "JUDGE_API_KEY  = OPENAI_API_KEY\n",
    "JUDGE_MODEL    = \"qwen.qwen3-32b-v1:0\"\n",
    "\n",
    "print(\"Judge base URL:\", JUDGE_BASE_URL)\n",
    "print(\"Judge model:   \", JUDGE_MODEL)\n",
    "\n",
    "judge_client = OpenAI(\n",
    "    base_url=JUDGE_BASE_URL,\n",
    "    api_key=JUDGE_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adea26b-532a-4681-b165-8e4ae24c9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper: safely pull JSON out of the judge’s response\n",
    "\n",
    "Sometimes a model wraps JSON in extra text (e.g., comments or backticks).  \n",
    "This helper function:\n",
    "\n",
    "- Looks for the first `{` and last `}` in the text.\n",
    "- Returns just that portion as a JSON string.\n",
    "- Raises a clear error if it can’t find a JSON object.\n",
    "\n",
    "We use this to be more forgiving if the judge model doesn’t respond with “perfect” JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3f40b8-5c77-4c59-9f7e-a16d0554e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: extract the first JSON object from a string (if judge wraps it)\n",
    "def extract_json_block(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Try to pull the first {...} JSON object out of a string.\n",
    "    Useful if the judge wraps JSON in extra text or backticks.\n",
    "    \"\"\"\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    if start == -1 or end == -1 or end <= start:\n",
    "        raise ValueError(f\"Could not find JSON object in: {text!r}\")\n",
    "    return text[start : end + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e0aadf-d503-4cf0-a2a3-48253bef5a31",
   "metadata": {},
   "source": [
    "### Call the judge model once for a single QA pair\n",
    "\n",
    "This cell defines `judge_single`, which:\n",
    "\n",
    "1. Builds a **system message** telling the judge to be an impartial evaluator.  \n",
    "2. Sends the judge:\n",
    "   - The **question**,\n",
    "   - The **context** (the supporting text),\n",
    "   - The **gold/reference answer** (what we believe is correct),\n",
    "   - The **model’s answer** (baseline or student).\n",
    "3. Asks the judge to return a small JSON object with scores (usually 1–5) for:\n",
    "   - **Coherence** – Is the answer logically consistent and well-structured?\n",
    "   - **Relevance** – Does it answer the question and stay on topic?\n",
    "   - **Groundedness** – Is it supported by the given context (not hallucinated)?\n",
    "   - **Fluency** – Is the language clear and readable?\n",
    "   - **Similarity** – How close is it to the reference/gold answer?\n",
    "\n",
    "The function:\n",
    "- Calls the judge via `judge_client.chat.completions.create`.\n",
    "- Parses the JSON (using `extract_json_block` if needed).\n",
    "- Returns a dictionary with standardized score names like `\"coherence.gpt_coherence\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b99831-82f6-4c70-b131-49867fadc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the judge model once to score a single QA pair\n",
    "def judge_single(question: str, context: str, gold: str, answer: str) -> dict:\n",
    "    \"\"\"\n",
    "    Call the judge model (Qwen via the router) to score one answer.\n",
    "\n",
    "    Returns a dict with keys like:\n",
    "      - \"coherence.gpt_coherence\"\n",
    "      - \"relevance.gpt_relevance\"\n",
    "      - \"groundedness.gpt_groundedness\"\n",
    "      - \"fluency.gpt_fluency\"\n",
    "      - \"similarity.gpt_similarity\"\n",
    "    \"\"\"\n",
    "\n",
    "    system_msg = (\n",
    "        \"You are an impartial evaluation assistant. \"\n",
    "        \"Given a question, context, reference answer, and model answer, \"\n",
    "        \"you will score the model answer on several dimensions from 1 to 5.\\n\\n\"\n",
    "        \"Return ONLY a valid JSON object, no commentary, in this format:\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"coherence\":   <number 1-5>,\\n'\n",
    "        '  \"relevance\":   <number 1-5>,\\n'\n",
    "        '  \"groundedness\":<number 1-5>,\\n'\n",
    "        '  \"fluency\":     <number 1-5>,\\n'\n",
    "        '  \"similarity\":  <number 1-5>\\n'\n",
    "        \"}\\n\"\n",
    "        \"Use whole or decimal numbers (e.g., 3 or 4.5).\"\n",
    "    )\n",
    "\n",
    "    user_msg = f\"\"\"\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context (information to base the answer on):\n",
    "{context}\n",
    "\n",
    "Reference answer (ground truth):\n",
    "{gold}\n",
    "\n",
    "Model answer to evaluate:\n",
    "{answer}\n",
    "\"\"\"\n",
    "\n",
    "    resp = judge_client.chat.completions.create(\n",
    "        model=JUDGE_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=256,\n",
    "    )\n",
    "\n",
    "    text = resp.choices[0].message.content.strip()\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        data = json.loads(extract_json_block(text))\n",
    "\n",
    "    # Map to the same naming style as the original RAFT/Azure notebook\n",
    "    scores = {\n",
    "        \"coherence.gpt_coherence\":       float(data[\"coherence\"]),\n",
    "        \"relevance.gpt_relevance\":       float(data[\"relevance\"]),\n",
    "        \"groundedness.gpt_groundedness\": float(data[\"groundedness\"]),\n",
    "        \"fluency.gpt_fluency\":           float(data[\"fluency\"]),\n",
    "        \"similarity.gpt_similarity\":     float(data[\"similarity\"]),\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8ddc28-3461-40da-9903-61640da93597",
   "metadata": {},
   "source": [
    "### Score a full dataset and compute row-level + overall metrics\n",
    "\n",
    "This cell defines `score_dataset_file`, which runs the judge over **all** examples in a formatted eval file:\n",
    "\n",
    "- **Input**:\n",
    "  - `formatted_answers_path` – the RAFT eval JSONL (baseline or student).\n",
    "  - `row_scores_output_path` – where to save per-question scores (JSONL).\n",
    "  - `metrics_output_path` – where to save overall averages (JSON).\n",
    "- **What it does**:\n",
    "  1. Loads all examples into a pandas DataFrame.\n",
    "  2. Loops through each row with a `tqdm` progress bar.\n",
    "  3. For each row, calls `judge_single(...)` to get the five scores.\n",
    "  4. Accumulates sums for each metric.\n",
    "  5. Stores a row-level record with:\n",
    "     - Inputs (question, context, gold answer, model answer),\n",
    "     - Outputs (the scores from the judge).\n",
    "  6. Writes:\n",
    "     - A **row-level scores JSONL** file (one line per question).\n",
    "     - An **aggregate metrics JSON** file with average scores.\n",
    "\n",
    "It then returns the metrics dictionary so we can use it in later cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62b189d-aca9-44d1-8001-eb6e341244b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score a full formatted dataset file and write row-level + aggregate metrics\n",
    "def score_dataset_file(\n",
    "    formatted_answers_path: str,\n",
    "    row_scores_output_path: str,\n",
    "    metrics_output_path: str,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    formatted_answers_path: JSONL from .gorilla/raft/format.py (eval format)\n",
    "    row_scores_output_path: JSONL where we write per-row inputs & outputs\n",
    "    metrics_output_path:    JSON file with aggregated mean scores\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_json(formatted_answers_path, lines=True)\n",
    "    print(f\"Loaded {len(df)} examples from {formatted_answers_path}\")\n",
    "\n",
    "    rows_out = []\n",
    "    metric_sums = None\n",
    "    n = 0\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        question = row[\"question\"]\n",
    "        context = row.get(\"context\", \"\")\n",
    "        gold    = row[\"gold_final_answer\"]\n",
    "        answer  = row[\"final_answer\"]\n",
    "\n",
    "        scores = judge_single(question, context, gold, answer)\n",
    "\n",
    "        # Initialize metric sums\n",
    "        if metric_sums is None:\n",
    "            metric_sums = {k: 0.0 for k in scores.keys()}\n",
    "\n",
    "        for k, v in scores.items():\n",
    "            metric_sums[k] += v\n",
    "\n",
    "        rows_out.append(\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                    \"question\":          question,\n",
    "                    \"context\":           context,\n",
    "                    \"final_answer\":      answer,\n",
    "                    \"gold_final_answer\": gold,\n",
    "                },\n",
    "                \"outputs\": scores,\n",
    "            }\n",
    "        )\n",
    "        n += 1\n",
    "\n",
    "    # Write row-level scores\n",
    "    with open(row_scores_output_path, \"w\") as f:\n",
    "        for row in rows_out:\n",
    "            f.write(json.dumps(row) + \"\\n\")\n",
    "    print(\"Wrote row scores to:\", row_scores_output_path)\n",
    "\n",
    "    # Compute aggregate metrics\n",
    "    metrics = {k: (v / n if n > 0 else 0.0) for k, v in metric_sums.items()}\n",
    "    with open(metrics_output_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(\"Wrote aggregate metrics to:\", metrics_output_path)\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2413242-2b4c-43e6-ba23-ad41e2e2e731",
   "metadata": {},
   "source": [
    "### Evaluate the baseline model with the judge\n",
    "\n",
    "This cell:\n",
    "\n",
    "- Imports `tqdm` and `json` for progress and file handling.\n",
    "- Calls `score_dataset_file` on the **baseline formatted answers**.\n",
    "- Prints the baseline’s average scores for each metric (coherence, relevance, etc.).\n",
    "\n",
    "These scores form our **reference point** – how well the teacher model performs on this eval set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0804914d-4cc4-4931-9cef-6808ef0608ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run judge over baseline formatted answers and compute metrics\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "baseline_metrics = score_dataset_file(\n",
    "    formatted_answers_path=dataset_path_eval_answer_baseline,\n",
    "    row_scores_output_path=dataset_path_eval_answer_score_baseline,\n",
    "    metrics_output_path=dataset_path_eval_answer_score_metrics_baseline,\n",
    ")\n",
    "\n",
    "print(\"\\nBaseline metrics (judge model):\")\n",
    "for k, v in baseline_metrics.items():\n",
    "    print(f\"{k}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6022dec4-6096-4694-92f2-b58ad1a02674",
   "metadata": {},
   "source": [
    "### Compare baseline vs student scores and compute improvement\n",
    "\n",
    "Below this cell:\n",
    "\n",
    "- Builds a small pandas table (`metrics_df`) with:\n",
    "  - One row per metric,\n",
    "  - Two columns: `baseline` and `student`.\n",
    "- Adds an `improvement` column that shows the **relative change**:\n",
    "  \\[\n",
    "  \\text{improvement} = \\frac{\\text{student} - \\text{baseline}}{\\text{baseline}}\n",
    "  \\]\n",
    "\n",
    "In plain terms:\n",
    "- A **positive** improvement value means the student scored higher than the baseline for that metric.\n",
    "- A **negative** value means the student did worse.\n",
    "\n",
    "The displayed table lets you quickly see where finetuning helped, hurt, or had little effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf00e1ea-330a-4cb2-86f0-d81ff29b345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run judge over student formatted answers and compute metrics\n",
    "student_metrics = score_dataset_file(\n",
    "    formatted_answers_path=dataset_path_eval_answer_student,\n",
    "    row_scores_output_path=dataset_path_eval_answer_score_student,\n",
    "    metrics_output_path=dataset_path_eval_answer_score_metrics_student,\n",
    ")\n",
    "\n",
    "print(\"\\nStudent metrics (judge model):\")\n",
    "for k, v in student_metrics.items():\n",
    "    print(f\"{k}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8df9ba-b985-4083-aac2-b29122be0424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs student metrics and compute relative improvement\n",
    "import pandas as pd\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"baseline\": baseline_metrics,\n",
    "        \"student\":  student_metrics,\n",
    "    }\n",
    ")\n",
    "\n",
    "metrics_df[\"improvement\"] = (metrics_df[\"student\"] - metrics_df[\"baseline\"]) / metrics_df[\"baseline\"]\n",
    "\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b7902c-781b-4428-99cd-1a7a39ab2fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
